<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[:HIPSTER_DEV_BLOG]]></title>
  <link href="http://Tim-B.github.io/hipsterdevblog.com/atom.xml" rel="self"/>
  <link href="http://Tim-B.github.io/hipsterdevblog.com/"/>
  <updated>2014-06-22T21:13:43+10:00</updated>
  <id>http://Tim-B.github.io/hipsterdevblog.com/</id>
  <author>
    <name><![CDATA[Tim B.]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Lazy Processing Images Using S3 and Redirection Rules]]></title>
    <link href="http://Tim-B.github.io/hipsterdevblog.com/blog/2014/06/22/lazy-processing-images-using-s3-and-redirection-rules/"/>
    <updated>2014-06-22T19:14:44+10:00</updated>
    <id>http://Tim-B.github.io/hipsterdevblog.com/blog/2014/06/22/lazy-processing-images-using-s3-and-redirection-rules</id>
    <content type="html"><![CDATA[<p>
    In a system dealing with user generated images it&#8217;s common to have to resize images before they can be served to the web.
    Storing and serving large quantities of user generated images can also be a challenge – that is unless you&#8217;re using
    AWS S3. A typical implementation using S3 to store and serve images requires images to be resized into every required
    size and saved to S3 upon being uploaded. An unfortunate limitation of this technique is that you must know all
     required sizes at the time the image is uploaded – something that may not be constant, consistent or known in some
     (particularly legacy) applications.
</p>


<p>
     One solution is to automatically resize images the first time they&#8217;re requested
     using dimensions provided in the image URL, this way the application requesting the image can choose an appropriate
     size. While S3 doesn&#8217;t provide functionality to transparently proxy image misses to your image processor, it is
     possible to use S3 <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/HowDoIWebsiteConfiguration.html">S3
     routing rules</a> to achieve a similar function.
</p>


<h2>Overview</h2>


<p>
    Using routing rules it&#8217;s possible to return a 302 redirect whenever a 404 error occurs, this 302 redirect can then
    take the user to your EC2 instance which resizes the image, serves it to them then saves the resized copy back to
     the original bucket so future visitors won&#8217;t be redirected.
</p>


<p><img class="left" src="http://Tim-B.github.io/hipsterdevblog.com/images/posts/s3_lazy_process/s3_route.png"></p>

<h2>Implementation</h2>


<p>
    First, it&#8217;s assumed that you have a bucket setup to serve its content publicly on one domain and your processing
    server on another. Both domains must use the same URL structure for images aside from the host name, so for example
     <code>images.domain.com/widgets/myimage_600_400.jpg</code> and <code>process.domain.com/widgets/myimage_600_400.jpg</code>
     should both work (assuming <code>images.mydomain.com</code> is the bucket and <code>process.mydomain.com</code> is the processor).
</p>


<p>
    When receiving a request <code>process.mydomain.com</code> should resize the image (most likely after obtaining the
    image from another private bucket for originals), resize it, serve that image to the visitor then save it back to
    the <code>images.mydomain.com</code> bucket.
</p>


<p>
    Next – go to the bucket in the S3 console, go to the bucket properties and enter the following routing rules
    in the &#8216;Enable website hosting&#8217; accordion menu:
</p>


<figure class='code'><figcaption><span>Routing.xml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;RoutingRules&gt;</span>
</span><span class='line'>    <span class="nt">&lt;RoutingRule&gt;</span>
</span><span class='line'>        <span class="nt">&lt;Condition&gt;</span>
</span><span class='line'>            <span class="nt">&lt;HttpErrorCodeReturnedEquals&gt;</span>404<span class="nt">&lt;/HttpErrorCodeReturnedEquals&gt;</span>
</span><span class='line'>        <span class="nt">&lt;/Condition&gt;</span>
</span><span class='line'>        <span class="nt">&lt;Redirect&gt;</span>
</span><span class='line'>            <span class="nt">&lt;HostName&gt;</span>process.domain.com<span class="nt">&lt;/HostName&gt;</span>
</span><span class='line'>            <span class="nt">&lt;HttpRedirectCode&gt;</span>302<span class="nt">&lt;/HttpRedirectCode&gt;</span>
</span><span class='line'>        <span class="nt">&lt;/Redirect&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/RoutingRule&gt;</span>
</span><span class='line'><span class="nt">&lt;/RoutingRules&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p><img src="http://Tim-B.github.io/hipsterdevblog.com/images/posts/s3_lazy_process/bucket_config.png"></p>

<p>
    You now have lazy image processing!
</p>


<h2>Enter CloudFront</h2>


<p>
    S3 is a reasonably effective CDN (in the sense that it offloads serving images), but it&#8217;s not geographically
    distributed and if you&#8217;re serving images to visitors across the globe you may wish to also implement CloudFront.
    Unfortunately you can&#8217;t simply setup CloudFront to use the <code>images.mydomain.com</code> bucket as an origin because
     CloudFront will cache the 302 redirects for a minimum of 60 minutes – meaning your image processor might process the same image many times.
</p>


<p>
    One solution is to put a second CloudFront distribution in front of <code>process.domain.com</code> and set the S3
    redirect to use that CloudFront endpoint rather than the processor directly. In this scenario the first region to receive
     a request will pass through the first CloudFront distribution, the S3 bucket, the second CloudFront distribution
     and then hit the processor. The second request from that same region should then hit the second CloudFront distribution
     as will every other request from this region until the 302 redirect expires.
     Users making requests from other regions after the first request won&#8217;t have the 302 redirect in the cache for their region
     so they should hit the file in the S3 bucket which will then get cached in their region.
</p>


<h2>Caveats</h2>


<p>
    While this approach is certainly effective in some scenarios it&#8217;s not exactly <em>elegant</em>.
    Be sure to first consider whether CloudFront alone or
    resizing images in advance would work better in your situation. It&#8217;s also worth keeping in mind that all genuine
    404 requests (that is for images which don&#8217;t exist at all) will get passed to your server and won&#8217;t be offloaded to S3.
</p>


<p>
    Be careful when lazy processing images in general – if you don&#8217;t implement some form of rate limiting you may
      end up being vulnerable to a Denial of Service attack if someone were to try and request thousands of images in
      different sizes.
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Retrieving Files From S3 Using Chef on OpsWorks]]></title>
    <link href="http://Tim-B.github.io/hipsterdevblog.com/blog/2014/06/22/retrieving-files-from-s3-using-chef-on-opsworks/"/>
    <updated>2014-06-22T17:44:02+10:00</updated>
    <id>http://Tim-B.github.io/hipsterdevblog.com/blog/2014/06/22/retrieving-files-from-s3-using-chef-on-opsworks</id>
    <content type="html"><![CDATA[<p>
    Say you wanted to manage some configuration file in your OpsWorks stack – typically you&#8217;d create a custom Chef recipe,
    make your configuration file a template and store it within your custom cookbook repository. This approach works well
    in most instances, but what if the file is something not suited to version control such as a large binary file or
    perhaps a programmatically generated artifact of your system?
</p>


<p>
    In these cases you may prefer to store the file in an S3 bucket and automatically download a copy of the file
    as part of a custom recipe. In my case I wanted to have a dynamically generated (by a separate system)
    vhost configuration file which could be deployed to a stack using a simple recipe.
</p>




<h2>Adding AWS cookbook via Berkshelf</h2>




<p>
    The first thing you&#8217;ll need to do is add the OpsCode <a href="http://community.opscode.com/cookbooks/aws">AWS
    cookbook</a> to your Berkfile. Note that Berkshelf is only supported on Chef 11.10 or higher on OpsWorks, so if your
    OpsWorks stack has an older version selected you&#8217;ll have to either upgrade or include the whole AWS cookbook in your
    custom cookbook repository.
</p>


<p>
    If you don&#8217;t already have a Berkfile you&#8217;ll need to create one in your custom cookbook repository, otherwise simply
    add the AWS cookbook. Your Berkfile should look something like this:
</p>


<figure class='code'><figcaption><span>Berksfile </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>source "https://api.berkshelf.com"
</span><span class='line'>
</span><span class='line'>cookbook "aws", ">= 2.2.2"</span></code></pre></td></tr></table></div></figure>




<h2>Creating an S3 bucket and a user which can access it</h2>


<p>
    You can probably figure out how to create a bucket on your own. In my case I have a bucket called &#8216;test-site-config&#8217;
    and a file in there called &#8216;vhost.map&#8217; which I want to download via Chef.
</p>


<p> <img src="http://Tim-B.github.io/hipsterdevblog.com/images/posts/s3_chef_opsworks/bucket.png"></p>

<p>
    Next you&#8217;ll need some AWS credentials for Chef to use while downloading the file. You can use your root account
    but I&#8217;d strongly suggest using an IAM user limited to your bucket instead. If you create a new IAM user you can
    use the following policy which will only permit reading objects from the specified S3 bucket (obviously replace
    &#8216;test-site-config&#8217; with your own bucket name:
</p>


<figure class='code'><figcaption><span>IAM policy </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  "Version": "2012-10-17",
</span><span class='line'>  "Statement": [
</span><span class='line'>    {
</span><span class='line'>      "Sid": "Stmt1403407152000",
</span><span class='line'>      "Effect": "Allow",
</span><span class='line'>      "Action": [
</span><span class='line'>        "s3:GetObject"
</span><span class='line'>      ],
</span><span class='line'>      "Resource": [
</span><span class='line'>        "arn:aws:s3:::test-site-config/*"
</span><span class='line'>      ]
</span><span class='line'>    }
</span><span class='line'>  ]
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>




<h2>Passing AWS credentials via custom JSON</h2>


<p>
    Now navigate to your stack in the OpsWorks console, click &#8216;Stack Settings&#8217; then &#8216;Edit&#8217; and modify the Custom JSON
    field to include variables for your access and secret key. If you already have custom JSON values then you&#8217;ll
    need to merge the new values with your existing JSON, otherwise you can use the code below:
</p>


<figure class='code'><figcaption><span>Custom JSON </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>  "custom_access_key": "&lt;insert access key>",
</span><span class='line'>  "custom_secret_key": "&lt;insert secret key>"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p><img src="http://Tim-B.github.io/hipsterdevblog.com/images/posts/s3_chef_opsworks/edit_stack.png"></p>

<h2>Creating your custom recipe</h2>


<p>
    In this instance I&#8217;ll create a new recipe called &#8216;deployfile&#8217; which does nothing but download my file and save it to the specified
    location, however you could just as easily include this code within an existing recipe.
</p>


<p>
    Create the following file structure and use the code below in your custom cookbook repository:
</p>


<figure class='code'><figcaption><span>File Structure </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>deployfile/metadata.rb
</span><span class='line'>deployfile/recipes/default.rb</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span>metadata.rb </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='rb'><span class='line'><span class="nb">name</span>        <span class="s2">&quot;deployfile&quot;</span>
</span><span class='line'><span class="n">description</span> <span class="s2">&quot;Deploy File From S3&quot;</span>
</span><span class='line'><span class="n">maintainer</span>  <span class="s2">&quot;Dilbert&quot;</span>
</span><span class='line'><span class="n">license</span>     <span class="s2">&quot;Apache 2.0&quot;</span>
</span><span class='line'><span class="n">version</span>     <span class="s2">&quot;1.0.0&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="n">depends</span> <span class="s2">&quot;aws&quot;</span>
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span>recipes/default.rb </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='rb'><span class='line'><span class="n">include_recipe</span> <span class="s1">&#39;aws&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="n">aws_s3_file</span> <span class="s2">&quot;/etc/apache2/vhost.map&quot;</span> <span class="k">do</span>
</span><span class='line'>  <span class="n">bucket</span> <span class="s2">&quot;test-site-config&quot;</span>
</span><span class='line'>  <span class="n">remote_path</span> <span class="s2">&quot;vhost.map&quot;</span>
</span><span class='line'>  <span class="n">aws_access_key_id</span> <span class="n">node</span><span class="o">[</span><span class="ss">:custom_access_key</span><span class="o">]</span>
</span><span class='line'>  <span class="n">aws_secret_access_key</span> <span class="n">node</span><span class="o">[</span><span class="ss">:custom_secret_key</span><span class="o">]</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>




<p>
    Substitute <code>/etc/apache2/vhost.map</code> with the destination on your nodes, the bucket name and the remote
    path as required. You can also use other attributes belonging to the <a href="http://docs.opscode.com/resource_file.html">Chef
    file resource</a>.
</p>




<h2>Updating stack and executing recipe</h2>


<p>Once the code above has been committed and pushed back to your repository you&#8217;re finally ready to execute the recipe.</p>




<p>Go to your stack and click &#8216;Run Command&#8217;, select &#8216;Update Custom Cookbooks&#8217;:</p>


<p><img src="http://Tim-B.github.io/hipsterdevblog.com/images/posts/s3_chef_opsworks/update_cookbook.png"></p>

<p>Once OpsWorks has finished updating your custom cookbooks go back to &#8216;Run Command&#8217; and select &#8216;Execute Recipes&#8217;.
Enter the name of your recipe into the &#8216;Recipes to execute&#8217; field:</p>


<p><img src="http://Tim-B.github.io/hipsterdevblog.com/images/posts/s3_chef_opsworks/deploy_file.png"></p>

<p>Alternatively you can add your recipe to a layer life-cycle event (such as setup) and execute that life-cycle event
instead</p>


<p>Once that recipe has finished executing the file downloaded from S3 should now be present on your system!</p>

]]></content>
  </entry>
  
</feed>
